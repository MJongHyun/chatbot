import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import ast
from collections import Counter
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
font_path = 'C:\\Windows\\Fonts\\gulim.ttc'
font = fm.FontProperties(fname=font_path).get_name()
plt.rc('font', family=font)

try:
    script_path = os.path.dirname(os.path.abspath(__file__))
    font_path = os.path.join(script_path, "fonts", "malgun.ttf") # 'malgun.ttf'ëŠ” ì‹¤ì œ í°íŠ¸ íŒŒì¼ëª…ìœ¼ë¡œ ë³€ê²½

    # 2. Matplotlibì— í°íŠ¸ ê²½ë¡œ ì§€ì • ë° ì ìš©
    font_prop = fm.FontProperties(fname=font_path)
    plt.rc('font', family=font_prop.get_name())
    plt.rc('axes', unicode_minus=False) # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

    st.success(f"í°íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤: {font_path}")

except FileNotFoundError:
    st.error(f"í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. '{font_path}' ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.info("í”„ë¡œì íŠ¸ í´ë”ì— 'fonts' í´ë”ë¥¼ ë§Œë“¤ê³  ê·¸ ì•ˆì— .ttf í°íŠ¸ íŒŒì¼ì„ ë„£ì—ˆëŠ”ì§€, íŒŒì¼ ì´ë¦„ì´ ì •í™•í•œì§€ í™•ì¸í•˜ì„¸ìš”.")
    st.stop() # í°íŠ¸ê°€ ì—†ìœ¼ë©´ ì•± ì‹¤í–‰ ì¤‘ì§€

# --- 1. ì•± ì„¤ì • ---

st.set_page_config(layout="wide")
st.title("ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ í’ˆì§ˆ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

# --- 2. ë°ì´í„° ì²˜ë¦¬ ë° ì‹œê°í™” í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ) ---
# ì´ì „ê³¼ ë™ì¼í•œ í•¨ìˆ˜ë“¤ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

def intent_v1_v2_true_false_res(cbot_res_df):
    df = cbot_res_df.copy()
    idxTFDF = pd.DataFrame()
    for idx in df['index'].tolist():
        mask = df['index'] == idx
        correct_TF_v2, correct_TF_v1 = df.loc[mask, 'correct_TF'].iloc[0], df.loc[mask, 'correct_TF_v1'].iloc[0]
        try:
            correct_TF_v2_list = set(ast.literal_eval(df.loc[mask, 'v2_int_code'].iloc[0]))
        except (ValueError, SyntaxError):
            correct_TF_v2_list = set()
        correct_TF_v1_list = df.loc[mask, 'qu_intent'].iloc[0]
        v1_DF = pd.DataFrame([correct_TF_v1_list], columns=['code']); v1_DF['index'] = idx; v1_DF['div'] = 'V1'; v1_DF['TF'] = correct_TF_v1
        if correct_TF_v2_list:
            v2_DF = pd.DataFrame(list(correct_TF_v2_list), columns=['code']); v2_DF['index'] = idx; v2_DF['div'] = 'V2'; v2_DF['TF'] = correct_TF_v2
            idxTFDF = pd.concat([idxTFDF, v1_DF, v2_DF], axis=0)
        else:
            idxTFDF = pd.concat([idxTFDF, v1_DF], axis=0)
    idxTFDF = idxTFDF.reset_index(drop=True)
    x_res_deno = idxTFDF[['code']].value_counts().reset_index(name='deno')
    x_res_nume = idxTFDF[idxTFDF['TF'] == True][['code']].value_counts().reset_index(name='nume')
    x_nume_deno = pd.merge(x_res_deno, x_res_nume, on='code', how='left').fillna(0)
    return idxTFDF, x_nume_deno

def scatter_plot_res_make(x_nume_deno):
    df = x_nume_deno.copy()
    if not df.empty:
        plot_df_final = df.rename(columns={'code': 'TypeName', 'deno': 'TotalInquiries', 'nume': 'CorrectAnswers'})
        plot_df_final['AccuracyRate'] = np.round(plot_df_final['CorrectAnswers'] / plot_df_final['TotalInquiries'] * 100, 2)
        return plot_df_final.drop_duplicates(subset=['TypeName']).reset_index(drop=True)
    return pd.DataFrame()

def res_excel_df_v1_v2_scatter(_cbot_row_idxTFDF, _cbot_scatter_df, _cbot_res_df, _cbot_answer_df, llm_judge_threshold):
    v2_row_res = _cbot_row_idxTFDF[_cbot_row_idxTFDF['div'] == 'V2']
    v2_all_code = pd.merge(_cbot_res_df, v2_row_res[['code', 'index']].drop_duplicates(), on='index')
    v2_all_code_g = np.round(v2_all_code.groupby('code')['v2_judge_mean_score'].mean(), 2).reset_index(name='all_v2_score')
    v2_code_g = v2_all_code_g.rename(columns={'code': 'TypeName'})
    r1 = pd.merge(_cbot_answer_df, v2_code_g, on='TypeName', how='inner')
    r2 = pd.merge(_cbot_scatter_df, r1, on='TypeName', how='inner')
    
    if r2.empty: return pd.DataFrame()
    median_inquiries = r2['TotalInquiries'].median()
    
    def assign_quadrant(row):
        if row['TotalInquiries'] > median_inquiries and row['all_v2_score'] > llm_judge_threshold: return '1ì‚¬ë¶„ë©´: ë§ìŒ/ë†’ìŒ'
        if row['TotalInquiries'] <= median_inquiries and row['all_v2_score'] > llm_judge_threshold: return '2ì‚¬ë¶„ë©´: ì ìŒ/ë†’ìŒ'
        if row['TotalInquiries'] <= median_inquiries and row['all_v2_score'] <= llm_judge_threshold: return '3ì‚¬ë¶„ë©´: ì ìŒ/ë‚®ìŒ'
        if row['TotalInquiries'] > median_inquiries and row['all_v2_score'] <= llm_judge_threshold: return '4ì‚¬ë¶„ë©´: ë§ìŒ/ë‚®ìŒ'
        return 'N/A'
    
    r2['Quadrant'] = r2.apply(assign_quadrant, axis=1)
    r2.rename(columns={'TypeName': 'ë¬¸ì„œì½”ë“œ', 'TotalInquiries': 'ì „ì²´ ì°¸ê³  ë¬¸ì„œ ìˆ˜', 'CorrectAnswers': 'LLM-Cor ì°¸ê³  ë¬¸ì„œ ìˆ˜', 'AccuracyRate': 'ì •í™•ë„', 'all_v2_score': 'LLM í‰ê·  LLM Judge', 'document_data': 'ë“±ë¡í•œ ë¬¸ì„œ', 'Quadrant': 'ì‚¬ë¶„ë©´ìœ í˜•'}, inplace=True)
    return r2[['ì‚¬ë¶„ë©´ìœ í˜•', 'ë¬¸ì„œì½”ë“œ', 'ì „ì²´ ì°¸ê³  ë¬¸ì„œ ìˆ˜', 'LLM-Cor ì°¸ê³  ë¬¸ì„œ ìˆ˜', 'ì •í™•ë„', 'LLM í‰ê·  LLM Judge', 'ë“±ë¡í•œ ë¬¸ì„œ']]

def interactive_scatter_plot(plot_df, service_name, llm_judge_score_median):
    median_inquiries = plot_df['ì „ì²´ ì°¸ê³  ë¬¸ì„œ ìˆ˜'].median()
    fig = go.Figure()
    quadrant_order = ['1ì‚¬ë¶„ë©´: ë§ìŒ/ë†’ìŒ', '2ì‚¬ë¶„ë©´: ì ìŒ/ë†’ìŒ', '3ì‚¬ë¶„ë©´: ì ìŒ/ë‚®ìŒ', '4ì‚¬ë¶„ë©´: ë§ìŒ/ë‚®ìŒ']
    colors = ['#2ca02c', '#1f77b4', '#ff7f0e', '#d62728']
    for quadrant_name, color in zip(quadrant_order, colors):
        subset = plot_df[plot_df['ì‚¬ë¶„ë©´ìœ í˜•'] == quadrant_name]
        if not subset.empty:
            hovertemplate = "<b>%{customdata[0]}</b><br>ì°¸ê³  ìˆ˜: %{x}<br>LLM ì ìˆ˜: %{y:.2f}<br>ì •í™•ë„: %{customdata[1]:.2f}%<extra></extra>"
            fig.add_trace(go.Scatter(x=subset['ì „ì²´ ì°¸ê³  ë¬¸ì„œ ìˆ˜'], y=subset['LLM í‰ê·  LLM Judge'], mode='markers', name=quadrant_name, marker=dict(size=12, color=color), customdata=np.stack((subset['ë¬¸ì„œì½”ë“œ'], subset['ì •í™•ë„']), axis=-1), hovertemplate=hovertemplate))
    fig.add_vline(x=median_inquiries, line_width=1.5, line_dash="dash", line_color="dimgray", annotation_text=f"ë¬¸ì„œ ìˆ˜ ì¤‘ì•™ê°’: {median_inquiries:.0f}")
    fig.add_hline(y=llm_judge_score_median, line_width=1.5, line_dash="dash", line_color="dimgray", annotation_text=f"ì ìˆ˜ ê¸°ì¤€ì„ : {llm_judge_score_median:.2f}")
    fig.update_layout(title=f'<b>{service_name} ë¬¸ì„œë³„ ë‹µë³€ í’ˆì§ˆ ë¶„ì„</b>', xaxis_title='ì°¸ê³ ëœ ë¬¸ì„œ ìˆ˜ (ê°œ)', yaxis_title='LLM Judge í‰ê·  ì ìˆ˜', legend_title_text='<b>ì‚¬ë¶„ë©´ ìœ í˜•</b>', height=600)
    st.plotly_chart(fig, use_container_width=True)

# --- 3. Streamlit ì•± ë©”ì¸ UI êµ¬ì„± ---

st.sidebar.title("ğŸ“„ íŒŒì¼ ì—…ë¡œë“œ")
# íŒŒì¼ ì—…ë¡œë“œ ë¡œì§ ë³€ê²½: ì´ 4ê°œì˜ íŒŒì¼ ì—…ë¡œë“œ
# cbot_answer_file = st.sidebar.file_uploader("1. ë¬¸ì„œ-ë‹µë³€ ì›ë³¸ íŒŒì¼ (cbot_answer)", type=['parquet'])
# c_df_file = st.sidebar.file_uploader("2. ì„±ê³¼ì¸¡ì • ê²°ê³¼ íŒŒì¼ (c_df)", type=['parquet'])
# qdf_file = st.sidebar.file_uploader("3. ì§ˆë¬¸ í´ëŸ¬ìŠ¤í„° ë¶„ì„ íŒŒì¼", type=['xlsx', 'csv', 'parquet'])
# keyword_file = st.sidebar.file_uploader("4. í‚¤ì›Œë“œ ë¶„ì„ íŒŒì¼", type=['xlsx', 'csv', 'parquet'])

cbot_answer_file = './c_bot_code_fin.parquet'
c_df_file =  './c_df_all.parquet'
qdf_file = r'C:\Users\MJH\Downloads\chatbot\chatbot_v2_test\llm_check_plus_rag\c_bot_test_cluster_res_0625.xlsx'
keyword_file = r'C:\Users\MJH\Downloads\chatbot\chatbot_v2_test\llm_check_plus_rag\c_bot_keyword_check_0625.xlsx'

# í—¬í¼ í•¨ìˆ˜: íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ë°ì´í„°í”„ë ˆì„ ë¡œë“œ
def load_dataframe(file_object):
    if file_object is None:
        return None
    file_name = file_object.name
    if file_name.endswith('.parquet'):
        return pd.read_parquet(file_object)
    elif file_name.endswith('.csv'):
        return pd.read_csv(file_object)
    elif file_name.endswith('.xlsx'):
        return pd.read_excel(file_object)
    else:
        st.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_name}")
        return None

# 4ê°œì˜ íŒŒì¼ì´ ëª¨ë‘ ì—…ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
# if cbot_answer_file and c_df_file and qdf_file and keyword_file:
# 1. íŒŒì¼ ë¡œë“œ
try:
    cbot_answer_df = pd.read_parquet(cbot_answer_file).rename(columns={'document_name': 'TypeName'})[['TypeName', 'document_data']]
    c_df = pd.read_parquet(c_df_file)
    qDF_refine = pd.read_excel(qdf_file)
    keywordAllDF = pd.read_excel(keyword_file)
except Exception as e:
    st.error(f"íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.stop()

# 2. ì‚°ì ë„ ìƒì„±
st.header("ğŸ“Š ë¬¸ì„œë³„ ë‹µë³€ í’ˆì§ˆ ê°œìš”")
st.info("ì•„ë˜ ì‚°ì ë„ëŠ” ê° ë¬¸ì„œì˜ 'ì°¸ê³ ëœ íšŸìˆ˜'ì™€ 'ë‹µë³€ í’ˆì§ˆ(LLM Judge ì ìˆ˜)'ì„ ì‹œê°í™”í•œ ê²ƒì…ë‹ˆë‹¤.")
col1, col2 = st.columns([1, 3])
with col1:
    llm_judge_score_input = st.number_input(
        label="LLM Judge ì ìˆ˜ ê¸°ì¤€ì„  ì„¤ì •", min_value=0.0, max_value=5.0, value=4.20, step=0.05, format="%.2f",
        help="ì´ ê°’ì„ ë³€ê²½í•˜ë©´ ì‚¬ë¶„ë©´ê³¼ ì ì˜ ìƒ‰ìƒì´ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³€ê²½ë©ë‹ˆë‹¤."
    )
cbot_row_idxTFDF, cbot_idxTFDF_pre = intent_v1_v2_true_false_res(c_df)
cbot_scatter_df_pre = scatter_plot_res_make(cbot_idxTFDF_pre)
final_df = res_excel_df_v1_v2_scatter(cbot_row_idxTFDF, cbot_scatter_df_pre, c_df, cbot_answer_df, llm_judge_score_input)
interactive_scatter_plot(final_df, 'ì±—ë´‡ ì„±ëŠ¥', llm_judge_score_input)

# 3. ìƒì„¸ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
st.header("ğŸ” ë‹µë³€ ì‹¤íŒ¨ ìƒì„¸ ë¶„ì„")
st.info("ì‚°ì ë„ì— í‘œì‹œëœ ë¬¸ì„œ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ë¯¸ë¦¬ ë¶„ì„ëœ 'ë‹µë³€ ì‹¤íŒ¨' ìƒì„¸ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")

if qDF_refine.empty:
    st.warning("ìƒì„¸ ë¶„ì„ ê²°ê³¼ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
else:
    doc_options = sorted(final_df['ë¬¸ì„œì½”ë“œ'].unique().tolist())
    selected_doc = st.selectbox("ìƒì„¸ ë¶„ì„ì„ ì›í•˜ëŠ” ë¬¸ì„œ ì½”ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:", options=doc_options, key="doc_selector")
    
    if selected_doc:
        try:
            doc_text = cbot_answer_df[cbot_answer_df['TypeName'] == selected_doc]['document_data'].iloc[0]
            with st.expander("ì„ íƒí•œ ë¬¸ì„œ ì›ë³¸ ë‚´ìš© ë³´ê¸°"):
                st.markdown(doc_text)
        except IndexError:
            st.warning("ì„ íƒí•œ ë¬¸ì„œì˜ ì›ë³¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        st.subheader(f"'{selected_doc}' ë¬¸ì„œì˜ ì£¼ìš” ì‹¤íŒ¨ í‚¤ì›Œë“œ")
        keywords_df = keywordAllDF[keywordAllDF['TypeName'] == selected_doc].sort_values('count', ascending=False).head(10)
        st.dataframe(keywords_df[['keyword', 'count']].rename(columns={'keyword':'í‚¤ì›Œë“œ', 'count':'ì–¸ê¸‰ íšŸìˆ˜'}))

        st.subheader(f"'{selected_doc}' ë¬¸ì„œì˜ ì‹¤íŒ¨ ì§ˆë¬¸ í´ëŸ¬ìŠ¤í„° ë¶„ì„")
        doc_detail_df = qDF_refine[qDF_refine['TypeName'] == selected_doc].copy()
        
        st.markdown("##### ğŸ”¢ ê²°ê³¼ í•„í„°ë§")
        filter_cols = st.columns(4)
        
        with filter_cols[0]:
            unique_clusters = doc_detail_df['cluster'].unique().tolist()
            selected_clusters = st.multiselect("ë¶„ì„ ê²°ê³¼ ì„ íƒ", options=unique_clusters, default=unique_clusters)
        with filter_cols[1]:
            min_semantic = st.number_input("ì˜ë¯¸ ìœ ì‚¬ë„ â‰¥", min_value=0.0, max_value=1.0, value=0.0, step=0.05, format="%.2f")
        with filter_cols[2]:
            min_lexical = st.number_input("ì–´íœ˜ ìœ ì‚¬ë„ â‰¥", min_value=0.0, max_value=1.0, value=0.0, step=0.05, format="%.2f")
        with filter_cols[3]:
            max_rank = st.number_input("í‰ê·  Rank â‰¤", min_value=0.0, value=10.0, step=0.5, format="%.1f")

        filtered_df = doc_detail_df[
            (doc_detail_df['cluster'].isin(selected_clusters)) &
            (doc_detail_df['semantic_score'] >= min_semantic) &
            (doc_detail_df['lexical_score'] >= min_lexical) &
            (doc_detail_df['question_cluster_rank'] <= max_rank)
        ]

        display_cols = {
            'question_cluster': 'ìœ ì‚¬ ì§ˆë¬¸ í´ëŸ¬ìŠ¤í„°', 'cluster': 'ë¶„ì„ ê²°ê³¼', 'semantic_score': 'ì˜ë¯¸ ìœ ì‚¬ë„', 
            'lexical_score': 'ì–´íœ˜ ìœ ì‚¬ë„', 'question_cluster_rank': 'í‰ê·  Rank', 'TRUE_CNT': 'ì •ë‹µ(T) ìˆ˜', 'FALSE_CNT': 'ì˜¤ë‹µ(F) ìˆ˜'
        }
        display_cols_exist = {k:v for k,v in display_cols.items() if k in filtered_df.columns}
        st.dataframe(filtered_df[display_cols_exist.keys()].rename(columns=display_cols_exist))

# else:
#     st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„ì— í•„ìš”í•œ íŒŒì¼ 4ê°œë¥¼ ëª¨ë‘ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
